
@article{sandler_mobilenetv2_2019,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	shorttitle = {{MobileNetV2}},
	url = {http://arxiv.org/abs/1801.04381},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.},
	urldate = {2020-12-29},
	journal = {arXiv:1801.04381 [cs]},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	month = mar,
	year = {2019},
	note = {arXiv: 1801.04381},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Sandler et al. - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:/home/juho/Zotero/storage/GWGEQJ7K/Sandler et al. - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:application/pdf},
}

@article{howard_mobilenets_2017,
	title = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
	shorttitle = {{MobileNets}},
	url = {http://arxiv.org/abs/1704.04861},
	abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depthwise separable convolutions to build light weight deep neural networks. We introduce two simple global hyperparameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
	urldate = {2020-12-29},
	journal = {arXiv:1704.04861 [cs]},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.04861},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:/home/juho/Zotero/storage/8DBN2JCK/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:application/pdf},
}

@article{howard_searching_2019,
	title = {Searching for {MobileNetV3}},
	url = {http://arxiv.org/abs/1905.02244},
	abstract = {We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardwareaware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 20\% compared to MobileNetV2. MobileNetV3-Small is 6.6\% more accurate compared to a MobileNetV2 model with comparable latency. MobileNetV3-Large detection is over 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LRASPP is 34\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.},
	urldate = {2020-12-29},
	journal = {arXiv:1905.02244 [cs]},
	author = {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and Le, Quoc V. and Adam, Hartwig},
	month = nov,
	year = {2019},
	note = {arXiv: 1905.02244},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Howard et al. - 2019 - Searching for MobileNetV3.pdf:/home/juho/Zotero/storage/XZK9FLXE/Howard et al. - 2019 - Searching for MobileNetV3.pdf:application/pdf},
}

@article{tan_efficientnet_2020,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://arxiv.org/abs/1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.},
	urldate = {2020-12-29},
	journal = {arXiv:1905.11946 [cs, stat]},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = sep,
	year = {2020},
	note = {arXiv: 1905.11946},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolu.pdf:/home/juho/Zotero/storage/9KZSJGDW/Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf},
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called {\textquotedblleft}dropout{\textquotedblright} that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	number = {6},
	urldate = {2020-12-29},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/home/juho/Zotero/storage/8AYQ8WNE/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf},
}

@book{frochte_maschinelles_2019,
	address = {M{\"u}nchen},
	edition = {2},
	title = {Maschinelles {Lernen}: {Grundlagen} und {Algorithmen} in {Python}},
	isbn = {978-3-446-45996-0 978-3-446-45997-7},
	shorttitle = {Maschinelles {Lernen}},
	url = {https://www.hanser-elibrary.com/doi/book/10.3139/9783446459977},
	urldate = {2020-12-29},
	publisher = {Carl Hanser Verlag GmbH \& Co. KG},
	author = {Frochte, J{\"o}rg},
	month = jan,
	year = {2019},
	doi = {10.3139/9783446459977},
}

@book{zhang_dive_2020,
	title = {Dive into {Deep} {Learning}},
	author = {Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
	year = {2020},
	file = {Zhang et al. - Dive into Deep Learning.pdf:/home/juho/Zotero/storage/U8CK4W2A/Zhang et al. - Dive into Deep Learning.pdf:application/pdf},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers{\textemdash}8{\texttimes} deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	urldate = {2021-01-02},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:/home/juho/Zotero/storage/XRLWXNCX/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@article{tan_mnasnet_2019,
	title = {{MnasNet}: {Platform}-{Aware} {Neural} {Architecture} {Search} for {Mobile}},
	shorttitle = {{MnasNet}},
	url = {http://arxiv.org/abs/1807.11626},
	abstract = {Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2\% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8{\texttimes} faster than MobileNetV2 [29] with 0.5\% higher accuracy and 2.3{\texttimes} faster than NASNet [36] with 1.2\% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/ tree/master/models/official/mnasnet.},
	urldate = {2021-01-02},
	journal = {arXiv:1807.11626 [cs]},
	author = {Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V.},
	month = may,
	year = {2019},
	note = {arXiv: 1807.11626},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Tan et al. - 2019 - MnasNet Platform-Aware Neural Architecture Search.pdf:/home/juho/Zotero/storage/7F323MK7/Tan et al. - 2019 - MnasNet Platform-Aware Neural Architecture Search.pdf:application/pdf},
}

@article{hu_squeeze-and-excitation_2019,
	title = {Squeeze-and-{Excitation} {Networks}},
	url = {http://arxiv.org/abs/1709.01507},
	abstract = {The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the {\textquotedblleft}Squeeze-and-Excitation{\textquotedblright} (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251\%, surpassing the winning entry of 2016 by a relative improvement of \~{}25\%. Models and code are available at https://github.com/hujie-frank/SENet.},
	urldate = {2021-01-03},
	journal = {arXiv:1709.01507 [cs]},
	author = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
	month = may,
	year = {2019},
	note = {arXiv: 1709.01507},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Hu et al. - 2019 - Squeeze-and-Excitation Networks.pdf:/home/juho/Zotero/storage/Y6BA8XGI/Hu et al. - 2019 - Squeeze-and-Excitation Networks.pdf:application/pdf},
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer{\textquoteright}s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2021-01-04},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Machine Learning},
	file = {Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:/home/juho/Zotero/storage/JJI8WHJS/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf},
}

@misc{liu_higher_2020,
	title = {Higher accuracy on vision models with {EfficientNet}-{Lite}},
	url = {https://blog.tensorflow.org/2020/03/higher-accuracy-on-vision-models-with-efficientnet-lite.html},
	abstract = {In May 2019, Google released a family of image classification models called EfficientNet, which achieved state-of-the-art accuracy with an order of magnitude of fewer computations and parameters. If EfficientNet can run on edge, it opens the door for novel applications on mobile and IoT where computational resources are constrained.},
	urldate = {2021-01-06},
	author = {Liu, Renjie},
	month = mar,
	year = {2020},
	file = {Snapshot:/home/juho/Zotero/storage/J5Y65YZS/higher-accuracy-on-vision-models-with-efficientnet-lite.html:text/html},
	note = {(accessed 6. Januar 2021)}
}

@article{mishra_survey_2020,
	title = {A {Survey} on {Deep} {Neural} {Network} {Compression}: {Challenges}, {Overview}, and {Solutions}},
	shorttitle = {A {Survey} on {Deep} {Neural} {Network} {Compression}},
	url = {http://arxiv.org/abs/2010.03954},
	abstract = {Deep Neural Network (DNN) has gained unprecedented performance due to its automated feature extraction capability. This high order performance leads to significant incorporation of DNN models in different Internet of Things (IoT) applications in the past decade. However, the colossal requirement of computation, energy, and storage of DNN models make their deployment prohibitive on resource constraint IoT devices. Therefore, several compression techniques were proposed in recent years for reducing the storage and computation requirements of the DNN model. These techniques on DNN compression have utilized a different perspective for compressing DNN with minimal accuracy compromise. It encourages us to make a comprehensive overview of the DNN compression techniques. In this paper, we present a comprehensive review of existing literature on compressing DNN model that reduces both storage and computation requirements. We divide the existing approaches into five broad categories, i.e., network pruning, sparse representation, bits precision, knowledge distillation, and miscellaneous, based upon the mechanism incorporated for compressing the DNN model. The paper also discussed the challenges associated with each category of DNN compression techniques. Finally, we provide a quick summary of existing work under each category with the future direction in DNN compression.},
	urldate = {2021-01-08},
	journal = {arXiv:2010.03954 [cs, eess]},
	author = {Mishra, Rahul and Gupta, Hari Prabhat and Dutta, Tanima},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.03954},
	keywords = {Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture, Electrical Engineering and Systems Science - Signal Processing},
	file = {Mishra et al. - 2020 - A Survey on Deep Neural Network Compression Chall.pdf:/home/juho/Zotero/storage/RN8I5WF4/Mishra et al. - 2020 - A Survey on Deep Neural Network Compression Chall.pdf:application/pdf},
}

@article{jacob_quantization_2017,
	title = {Quantization and {Training} of {Neural} {Networks} for {Efficient} {Integer}-{Arithmetic}-{Only} {Inference}},
	url = {http://arxiv.org/abs/1712.05877},
	abstract = {The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient and accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. As a result, the proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs.},
	urldate = {2021-01-08},
	journal = {arXiv:1712.05877 [cs, stat]},
	author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.05877},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Jacob et al. - 2017 - Quantization and Training of Neural Networks for E.pdf:/home/juho/Zotero/storage/TLCYKYS7/Jacob et al. - 2017 - Quantization and Training of Neural Networks for E.pdf:application/pdf},
}

@article{sheng_quantization-friendly_2018,
	title = {A {Quantization}-{Friendly} {Separable} {Convolution} for {MobileNets}},
	url = {http://arxiv.org/abs/1803.08607},
	doi = {10.1109/EMC2.2018.00011},
	abstract = {As deep learning (DL) is being rapidly pushed to edge computing, researchers invented various ways to make inference computation more efficient on mobile/IoT devices, such as network pruning, parameter compression, and etc. Quantization, as one of the key approaches, can effectively offload GPU, and make it possible to deploy DL on fixed-point pipeline. Unfortunately, not all existing networks design are friendly to quantization. For example, the popular lightweight MobileNetV1 [1], while it successfully reduces parameter size and computation latency with separable convolution, our experiment shows its quantized models have large accuracy gap against its float point models. To resolve this, we analyzed the root cause of quantization loss and proposed a quantization-friendly separable convolution architecture. By evaluating the image classification task on ImageNet2012 dataset, our modified MobileNetV1 model can archive 8-bit inference top-1 accuracy in 68.03\%, almost closed the gap to the float pipeline.},
	urldate = {2021-01-08},
	journal = {2018 1st Workshop on Energy Efficient Machine Learning and Cognitive Computing for Embedded Applications (EMC2)},
	author = {Sheng, Tao and Feng, Chen and Zhuo, Shaojie and Zhang, Xiaopeng and Shen, Liang and Aleksic, Mickey},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.08607},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {14--18},
	file = {Sheng et al. - 2018 - A Quantization-Friendly Separable Convolution for .pdf:/home/juho/Zotero/storage/4QJ2D99H/Sheng et al. - 2018 - A Quantization-Friendly Separable Convolution for .pdf:application/pdf},
}

@article{hooker_what_2020,
	title = {What {Do} {Compressed} {Deep} {Neural} {Networks} {Forget}?},
	url = {http://arxiv.org/abs/1911.05248},
	abstract = {Deep neural network pruning and quantization techniques have demonstrated it is possible to achieve high levels of compression with surprisingly little degradation to test set accuracy. However, this measure of performance conceals significant differences in how different classes and images are impacted by model compression techniques. We find that models with radically different numbers of weights have comparable top-line performance metrics but diverge considerably in behavior on a narrow subset of the dataset. This small subset of data points, which we term Pruning Identified Exemplars (PIEs) are systematically more impacted by the introduction of sparsity. Compression disproportionately impacts model performance on the underrepresented long-tail of the data distribution. PIEs over-index on atypical or noisy images that are far more challenging for both humans and algorithms to classify. Our work provides intuition into the role of capacity in deep neural networks and the trade-offs incurred by compression. An understanding of this disparate impact is critical given the widespread deployment of compressed models in the wild.},
	urldate = {2021-01-08},
	journal = {arXiv:1911.05248 [cs, stat]},
	author = {Hooker, Sara and Courville, Aaron and Clark, Gregory and Dauphin, Yann and Frome, Andrea},
	month = jul,
	year = {2020},
	note = {arXiv: 1911.05248},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {Hooker et al. - 2020 - What Do Compressed Deep Neural Networks Forget.pdf:/home/juho/Zotero/storage/WKX4LPWZ/Hooker et al. - 2020 - What Do Compressed Deep Neural Networks Forget.pdf:application/pdf},
}

@article{zhu_prune_2017,
	title = {To prune, or not to prune: exploring the efficacy of pruning for model compression},
	shorttitle = {To prune, or not to prune},
	url = {http://arxiv.org/abs/1710.01878},
	abstract = {Model pruning seeks to induce sparsity in a deep neural network{\textquoteright}s various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015a; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model{\textquoteright}s dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
	urldate = {2021-01-11},
	journal = {arXiv:1710.01878 [cs, stat]},
	author = {Zhu, Michael and Gupta, Suyog},
	month = nov,
	year = {2017},
	note = {arXiv: 1710.01878},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Zhu and Gupta - 2017 - To prune, or not to prune exploring the efficacy .pdf:/home/juho/Zotero/storage/N4VSIJB7/Zhu and Gupta - 2017 - To prune, or not to prune exploring the efficacy .pdf:application/pdf},
}

@article{gale_state_2019,
	title = {The {State} of {Sparsity} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1902.09574},
	abstract = {We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer trained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet. Across thousands of experiments, we demonstrate that complex techniques (Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression rates on smaller datasets perform inconsistently, and that simple magnitude pruning approaches achieve comparable or better results. Based on insights from our experiments, we achieve a new state-of-the-art sparsity-accuracy trade-off for ResNet-50 using only magnitude pruning. Additionally, we repeat the experiments performed by Frankle \& Carbin (2018) and Liu et al. (2018) at scale and show that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization. Together, these results highlight the need for large-scale benchmarks in the field of model compression. We open-source our code, top performing model checkpoints, and results of all hyperparameter configurations to establish rigorous baselines for future work on compression and sparsification.},
	urldate = {2021-01-11},
	journal = {arXiv:1902.09574 [cs, stat]},
	author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.09574},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Gale et al. - 2019 - The State of Sparsity in Deep Neural Networks.pdf:/home/juho/Zotero/storage/2C6AYGNN/Gale et al. - 2019 - The State of Sparsity in Deep Neural Networks.pdf:application/pdf},
}

@article{guo_survey_2018,
	title = {A {Survey} on {Methods} and {Theories} of {Quantized} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1808.04752},
	abstract = {Deep neural networks are the state-of-the-art methods for many real-world tasks, such as computer vision, natural language processing and speech recognition. For all its popularity, deep neural networks are also criticized for consuming a significant amount of memory and draining battery life of devices during training and inference. This makes it hard to deploy these models on mobile or embedded devices which have tight resource constraints. Quantization is recognized as one of the most effective approaches to satisfy the extreme memory requirements that deep neural network models demand. Instead of adopting 32-bit floating point format to represent weights, quantized representations store weights using more compact formats such as integers or even binary numbers. Despite a possible degradation in predictive performance, quantization provides a potential solution to greatly reduce the model size and the energy consumption. In this survey, we give a thorough review of different aspects of quantized neural networks. Current challenges and trends of quantized neural networks are also discussed.},
	urldate = {2021-01-12},
	journal = {arXiv:1808.04752 [cs, stat]},
	author = {Guo, Yunhui},
	month = dec,
	year = {2018},
	note = {arXiv: 1808.04752},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Guo - 2018 - A Survey on Methods and Theories of Quantized Neur.pdf:/home/juho/Zotero/storage/UDYYGHBA/Guo - 2018 - A Survey on Methods and Theories of Quantized Neur.pdf:application/pdf},
}

@misc{noauthor_flatbuffers_nodate,
	title = {{FlatBuffers}: {FlatBuffers}},
	url = {https://google.github.io/flatbuffers/index.html},
	urldate = {2021-01-25},
	file = {FlatBuffers\: FlatBuffers:/home/juho/Zotero/storage/ALGEVG9R/index.html:text/html},
}

@misc{noauthor_tensorflow_nodate,
	title = {{TensorFlow}},
	url = {https://www.tensorflow.org/},
	abstract = {An end-to-end open source machine learning platform for everyone. Discover TensorFlow's flexible ecosystem of tools, libraries and community resources.},
	urldate = {2021-01-25},
	file = {Snapshot:/home/juho/Zotero/storage/SACJVAG4/www.tensorflow.org.html:text/html},
}

@techreport{krizhevsky_learning_2009,
	title = {Learning {Multiple} {Layers} of {Features} from {Tiny} {Images}},
	author = {Krizhevsky, Alex},
	year = {2009},
	pages = {60},
	file = {Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf:/home/juho/Zotero/storage/MUC4PA68/Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf:application/pdf},
}

@article{blalock_what_2020,
	title = {What is the {State} of {Neural} {Network} {Pruning}?},
	url = {http://arxiv.org/abs/2003.03033},
	abstract = {Neural network pruning{\textemdash}the task of reducing the size of a network by removing parameters{\textemdash}has been the subject of a great deal of work in recent years. We provide a meta-analysis of the literature, including an overview of approaches to pruning and consistent findings in the literature. After aggregating results across 81 papers and pruning hundreds of models in controlled conditions, our clearest finding is that the community suffers from a lack of standardized benchmarks and metrics. This deficiency is substantial enough that it is hard to compare pruning techniques to one another or determine how much progress the field has made over the past three decades. To address this situation, we identify issues with current practices, suggest concrete remedies, and introduce ShrinkBench, an open-source framework to facilitate standardized evaluations of pruning methods. We use ShrinkBench to compare various pruning techniques and show that its comprehensive evaluation can prevent common pitfalls when comparing pruning methods.},
	urldate = {2021-01-27},
	journal = {arXiv:2003.03033 [cs, stat]},
	author = {Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.03033},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Blalock et al. - 2020 - What is the State of Neural Network Pruning.pdf:/home/juho/Zotero/storage/KYT5S4SA/Blalock et al. - 2020 - What is the State of Neural Network Pruning.pdf:application/pdf},
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	volume = {115},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-015-0816-y},
	doi = {10.1007/s11263-015-0816-y},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5~years of the challenge, and propose future directions and improvements.},
	number = {3},
	urldate = {2021-01-28},
	journal = {International Journal of Computer Vision},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = dec,
	year = {2015},
	pages = {211--252},
	file = {Springer Full Text PDF:/home/juho/Zotero/storage/IMB7KVKN/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:application/pdf},
}

@inproceedings{tu_pruning_2020,
	title = {Pruning {Depthwise} {Separable} {Convolutions} for {MobileNet} {Compression}},
	doi = {10.1109/IJCNN48605.2020.9207259},
	abstract = {Deep convolutional neural networks are good at accuracy while bad at efficiency. To improve the inference speed, two directions have been explored in the past, lightweight model designing and network weight pruning. Lightweight models have been proposed to improve the speed with good enough accuracy. It is, however, not trivial if we can further speed up these "compact" models by weight pruning. In this paper, we present a technique to gradually prune the depthwise separable convolution networks, such as MobileNet, for improving the speed of this kind of "dense" network. When pruning depthwise separable convolutions, we need to consider more structural constraints to ensure the speedup of inference. Instead of pruning the model with the desired ratio in one stage, the proposed multi-stage gradual pruning approach can stably prune the filters with a finer pruning ratio. Our method achieves satisfiable speedup with little accuracy drop for MobileNets. Code is available at https://github.com/ivclab/Multistage\_Pruning.},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Tu, C.-H. and Lee, J.-H. and Chan, Y.-M. and Chen, C.-S.},
	month = jul,
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {Convolutional codes, convolutional neural nets, convolutional neural networks, data compression, dense network, depthwise separable convolution networks, Indexes, inference speed, Information science, Kernel, Lightweight CNN, lightweight model, mobile computing, mobilenet compression, multistage gradual pruning approach, Network Pruning, network weight pruning, Neural networks, pruning depthwise separable convolutions, Standards, Task analysis},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:/home/juho/Zotero/storage/V4KZQ72G/9207259.html:text/html},
}
