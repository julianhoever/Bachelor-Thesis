\chapter{Zusammenfassung und Ausblick}
In diesem Kapitel wird diese Bachelorarbeit abgeschlossen und der Inhalt zusammengefasst. Außerdem wird ein Ausblick auf mögliche weiterführende Arbeiten an dieser Arbeit gegeben.



\section{Zusammenfassung}
Die Zielsetzung dieser Arbeit war die Auswirkungen der Optimierungstechniken Quantisierung und Pruning auf Netzwerkarchitekturen speziell für mobile und eingebettete Anwendungsbereiche zu untersuchen. Für diese Untersuchung musste ein geeignetes Evaluationsverfahren entwickelt werden.

Um die Auswirkungen der Optimierungstechniken auf Netzwerkarchitekturen für mobile und eingebettete Systeme zu untersuchen, muss in erster Linie eine Auswahl zu betrachtender Architekturen getroffen werden und dessen architekturellen Besonderheiten beleuchtet werden. In dieser Arbeit wurden in Kapitel \ref{grundlagen} die Architekturen MobileNet, MobileNetV2, MobileNetV3 und das EfficientNet genauer beschrieben. Außerdem wurde in diesem Kapitel auch besprochen, wie das von TensorFlow implementierte Quantisierungsschema funktioniert und was ein Magnitude Pruning bedeutet.

Nachdem die benötigten Grundlagen besprochen wurden, wurde in Kapitel \ref{ansatz_und_implementierung} das Vorgehen genauer beleuchtet, mit dem die Auswirkungen von Quantisierung und Pruning auf die Netzwerkarchitekturen untersucht werden sollten. Außerdem wurde vorgestellt wie Training, Quantisierung und Pruning mittels der Python-Bibliothek TensorFlow auf die verschiedenen Architekturen angewendet worden ist. Dabei wurde besonders auf eine einheitliche Parameterwahl in den Schritten Training, Quantisierung und Pruning geachtet, um die Architekturen zum Schluss besser miteinander vergleichen zu können. Diese einheitliche Parameterwahl hat aber, wie sich später herausgestellt hat, möglicherweise zu einem suboptimalen Training der MobileNetV3 Architekturen geführt. Zusätzlich war es bei den MobileNetV3 und EfficientNet Architekturen nötig einige Schichten vom Pruning auszuschließen, um ein Pruning dieser Architekturen in TensorFlow durchführen zu können.

Nachdem alle Architekturen dem Vorgehen entsprechend trainiert und optimiert wurden, folgte in Kapitel \ref{evaluation} die Evaluation der trainierten Modelle. Dazu wurden zu Beginn 5 Metriken definiert, welche teilweise auf dem Raspberry Pi 4 als exemplarische Plattform für mobile/eingebettete Anwendungen ausgewertet wurden. Die gewählten Metriken waren: Top-1/Top-3 Genauigkeit, Parameterzahl, Bedarf an Hintergrundspeicher, Bedarf an Hauptspeicher und Inferenzzeit. Mittels diesen Metriken sollten die Auswirkungen der Optimierungstechniken Quantisierung und Pruning gemessen werden. Was grundsätzlich nach dem Training der Architekturen aufgefallen ist, war, dass die Squeeze-And-Excitation Module, besonders bei der EfficientNet-B0 Architektur, zu einer deutlich erhöhten Inferenzzeit geführt haben. Zu der Quantisierung lässt sich zusammenfassen, dass eine Post-Training Float Fallback Quantisierung dieser vorgestellten Netzwerkarchitekturen durchaus sehr sinnvoll sein kann, da sie zu einer deutlichen Reduzierung der Modellgröße und sogar in einigen Fällen zu einer Verbesserung der Genauigkeit der quantisierten Modelle führt. Beim Pruning hat sich im Wesentlichen herausgestellt, dass ein Magnitude Pruning mit zunehmender Sparsity häufig die Genauigkeit der Modelle verschlechtert, sonst aber keine direkten Auswirkungen auf die restlichen Metriken hat. Wird jedoch auf die geprunten Modelle ein Kompressionsalgorithmus angewendet, kann die Größe der Modelle durch das Pruning stark reduziert werden. Werden die geprunten Modelle vor der Kompression noch quantisiert, kann die Modellgröße noch weiter reduziert werden. Zusätzlich zu den bis dahin betrachteten Metriken wurde in diesem Kapitel auch noch die klassenweise Genauigkeit als wichtige Metrik vorgestellt, um die ungleichmäßige Auswirkung von Quantisierung und Pruning auf die Genauigkeit der einzelnen Klassen zu untersuchen und exemplarisch an 2 Architekturen demonstriert. Danach wurden noch am Beispiel der MobileNetV3 Minimalistc Architekturen mögliche architekturelle Optimierungen untersucht und diskutiert, um die Inferenzzeiten und Parameterzahl der Architekturen zu minimieren und eine bessere Quantisierbarkeit zu gewährleisten. Zum Schluss wurde noch diskutiert welche Architekturen sich am besten für die Aufgabe der Bildklassifikation auf dem CIFAR-10 Datensatz eignen. Dabei hat sich herausgestellt, dass sich die MobileNet Architektur, trotz ihres Alters, immer noch sehr gut für diese Aufgabe eignet.



\section{Ausblick}
Während der Erstellung dieser Arbeit sind einige Stellen aufgetaucht, welche ein Startpunkt für weiterführende Arbeit sein können und welche in dieser Arbeit nicht näher beleuchtet wurden. In diesem Abschnitt sollen genau diese Startpunkte für weiterführende Arbeit diskutiert werden.

In dieser Arbeit wurde auf das Prinzip gesetzt, alle Architekturen mit denselben Parametern zu trainieren und zu optimieren. Dies sollte für vergleichbare Ergebnisse sorgen. Jedoch hat sich im Verlauf der Arbeit herausgestellt, dass dies problematisch sein kann, da beispielsweise die MobileNetV3 Architektur nach dem Training schlechte Ergebnisse erzielt. Außerdem bricht besonders die Genauigkeit der MobileNetV3 Architektur bei der Quantisierung sehr stark ein, was eine Folge des schlechten Trainings sein kann. Eine weiterführende Arbeit könnte sein von diesem einheitlichen Setup abzuweichen und für alle Architekturen einzeln möglichst optimale Parameter für das Training und die Optimierungen zu finden. Dadurch könnte ein realistischeres Bild von den Auswirkungen der Optimierungstechniken Quantisierung und Pruning auf die vorgestellten Architekturen entstehen.

Eine weitere Sache, die in dieser Arbeit aus zeitlichen Gründen nicht weiter betrachtet wurde, ist das Quantization-Aware Training. TensorFlow bietet dazu in dem TensorFlow Model Optimization Toolkit eine Implementierung \footnote{\url{https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/quantization/keras}} dafür. Es könnte interessant sein wie sich ein Quantization-Aware Training im Gegensatz zu der Post-Training Quantisierung auf die mobilen/eingebetteten Netzwerkarchitekturen auswirkt. 

In Bezug auf Quantisierung wurde in dieser Arbeit ausschließlich die Post-Training Float Fallback Quantisierung betrachtet, bei der alle Gewichte und Aktivierungen in 8 Bit Integer quantisiert werden. Jedoch bleibt bei dieser Methode die Eingabe und Ausgabe in 32 Bit Fließkommazahlen. Damit ist diese Art der Quantisierung ungeeignet für eine reine Integer-Only Hardware. Aus diesem Grund könnte in einer weiterführenden Arbeit die Post-Training Integer-Only Quantisierung genauer betrachtet werden, bei der auch die Eingabe und Ausgabe in 8 Bit Integer erfolgt.

Zusätzlich könnte mehr Arbeit investiert werden, um architekturelle Veränderungen an den vorgestellten Netzwerkarchitekturen vorzunehmen, um die Genauigkeit der quantisierten Modelle zu erhöhen. Dazu gibt es bereits Paper, in denen das gemacht wurde \cite{sheng_quantization-friendly_2018}.

Außerdem wurde in dieser Arbeit lediglich der Ansatz des Magnitude Pruning aus dem Jahr 2017 beschrieben \cite{zhu_prune_2017}. Jedoch gibt es eine Vielzahl weiterer Pruning Methoden \cite{gale_state_2019, mishra_survey_2020, tu_pruning_2020}, die auf die vorgestellten Netzwerkarchitekturen angewendet werden könnten und möglicherweise bessere Ergebnisse erzielen könnten als das Magnitude Pruning.
