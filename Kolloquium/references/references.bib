
@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called {\textquotedblleft}dropout{\textquotedblright} that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	number = {6},
	urldate = {2020-12-29},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/home/juho/Zotero/storage/8AYQ8WNE/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf},
} 
 
@article{howard_mobilenets_2017,
	title = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
	shorttitle = {{MobileNets}},
	url = {http://arxiv.org/abs/1704.04861},
	abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depthwise separable convolutions to build light weight deep neural networks. We introduce two simple global hyperparameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
	urldate = {2020-12-29},
	journal = {arXiv:1704.04861 [cs]},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.04861},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:/home/juho/Zotero/storage/8DBN2JCK/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:application/pdf},
}

@article{sandler_mobilenetv2_2019,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	shorttitle = {{MobileNetV2}},
	url = {http://arxiv.org/abs/1801.04381},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.},
	urldate = {2020-12-29},
	journal = {arXiv:1801.04381 [cs]},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	month = mar,
	year = {2019},
	note = {arXiv: 1801.04381},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Sandler et al. - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:/home/juho/Zotero/storage/GWGEQJ7K/Sandler et al. - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:application/pdf},
}

@article{howard_searching_2019,
	title = {Searching for {MobileNetV3}},
	url = {http://arxiv.org/abs/1905.02244},
	abstract = {We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardwareaware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 20\% compared to MobileNetV2. MobileNetV3-Small is 6.6\% more accurate compared to a MobileNetV2 model with comparable latency. MobileNetV3-Large detection is over 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LRASPP is 34\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.},
	urldate = {2020-12-29},
	journal = {arXiv:1905.02244 [cs]},
	author = {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and Le, Quoc V. and Adam, Hartwig},
	month = nov,
	year = {2019},
	note = {arXiv: 1905.02244},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Howard et al. - 2019 - Searching for MobileNetV3.pdf:/home/juho/Zotero/storage/XZK9FLXE/Howard et al. - 2019 - Searching for MobileNetV3.pdf:application/pdf},
}

@article{tan_efficientnet_2020,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://arxiv.org/abs/1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.},
	urldate = {2020-12-29},
	journal = {arXiv:1905.11946 [cs, stat]},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = sep,
	year = {2020},
	note = {arXiv: 1905.11946},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolu.pdf:/home/juho/Zotero/storage/9KZSJGDW/Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf},
}

@article{hu_squeeze-and-excitation_2019,
	title = {Squeeze-and-{Excitation} {Networks}},
	url = {http://arxiv.org/abs/1709.01507},
	abstract = {The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the {\textquotedblleft}Squeeze-and-Excitation{\textquotedblright} (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251\%, surpassing the winning entry of 2016 by a relative improvement of \~{}25\%. Models and code are available at https://github.com/hujie-frank/SENet.},
	urldate = {2021-01-03},
	journal = {arXiv:1709.01507 [cs]},
	author = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
	month = may,
	year = {2019},
	note = {arXiv: 1709.01507},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Hu et al. - 2019 - Squeeze-and-Excitation Networks.pdf:/home/juho/Zotero/storage/Y6BA8XGI/Hu et al. - 2019 - Squeeze-and-Excitation Networks.pdf:application/pdf},
}

@misc{liu_higher_2020,
	title = {Higher accuracy on vision models with {EfficientNet}-{Lite}},
	url = {https://blog.tensorflow.org/2020/03/higher-accuracy-on-vision-models-with-efficientnet-lite.html},
	abstract = {In May 2019, Google released a family of image classification models called EfficientNet, which achieved state-of-the-art accuracy with an order of magnitude of fewer computations and parameters. If EfficientNet can run on edge, it opens the door for novel applications on mobile and IoT where computational resources are constrained.},
	urldate = {2021-01-06},
	author = {Liu, Renjie},
	month = mar,
	year = {2020},
	file = {Snapshot:/home/juho/Zotero/storage/J5Y65YZS/higher-accuracy-on-vision-models-with-efficientnet-lite.html:text/html},
	note = {(accessed 6. Januar 2021)}
}

@article{jacob_quantization_2017,
	title = {Quantization and {Training} of {Neural} {Networks} for {Efficient} {Integer}-{Arithmetic}-{Only} {Inference}},
	url = {http://arxiv.org/abs/1712.05877},
	abstract = {The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient and accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. As a result, the proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs.},
	urldate = {2021-01-08},
	journal = {arXiv:1712.05877 [cs, stat]},
	author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.05877},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Jacob et al. - 2017 - Quantization and Training of Neural Networks for E.pdf:/home/juho/Zotero/storage/TLCYKYS7/Jacob et al. - 2017 - Quantization and Training of Neural Networks for E.pdf:application/pdf},
}

@article{zhu_prune_2017,
	title = {To prune, or not to prune: exploring the efficacy of pruning for model compression},
	shorttitle = {To prune, or not to prune},
	url = {http://arxiv.org/abs/1710.01878},
	abstract = {Model pruning seeks to induce sparsity in a deep neural network{\textquoteright}s various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015a; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model{\textquoteright}s dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
	urldate = {2021-01-11},
	journal = {arXiv:1710.01878 [cs, stat]},
	author = {Zhu, Michael and Gupta, Suyog},
	month = nov,
	year = {2017},
	note = {arXiv: 1710.01878},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Zhu and Gupta - 2017 - To prune, or not to prune exploring the efficacy .pdf:/home/juho/Zotero/storage/N4VSIJB7/Zhu and Gupta - 2017 - To prune, or not to prune exploring the efficacy .pdf:application/pdf},
}

@article{hooker_what_2020,
	title = {What {Do} {Compressed} {Deep} {Neural} {Networks} {Forget}?},
	url = {http://arxiv.org/abs/1911.05248},
	abstract = {Deep neural network pruning and quantization techniques have demonstrated it is possible to achieve high levels of compression with surprisingly little degradation to test set accuracy. However, this measure of performance conceals significant differences in how different classes and images are impacted by model compression techniques. We find that models with radically different numbers of weights have comparable top-line performance metrics but diverge considerably in behavior on a narrow subset of the dataset. This small subset of data points, which we term Pruning Identified Exemplars (PIEs) are systematically more impacted by the introduction of sparsity. Compression disproportionately impacts model performance on the underrepresented long-tail of the data distribution. PIEs over-index on atypical or noisy images that are far more challenging for both humans and algorithms to classify. Our work provides intuition into the role of capacity in deep neural networks and the trade-offs incurred by compression. An understanding of this disparate impact is critical given the widespread deployment of compressed models in the wild.},
	urldate = {2021-01-08},
	journal = {arXiv:1911.05248 [cs, stat]},
	author = {Hooker, Sara and Courville, Aaron and Clark, Gregory and Dauphin, Yann and Frome, Andrea},
	month = jul,
	year = {2020},
	note = {arXiv: 1911.05248},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {Hooker et al. - 2020 - What Do Compressed Deep Neural Networks Forget.pdf:/home/juho/Zotero/storage/WKX4LPWZ/Hooker et al. - 2020 - What Do Compressed Deep Neural Networks Forget.pdf:application/pdf},
}

@article{jacob_quantization_2017,
	title = {Quantization and {Training} of {Neural} {Networks} for {Efficient} {Integer}-{Arithmetic}-{Only} {Inference}},
	url = {http://arxiv.org/abs/1712.05877},
	abstract = {The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient and accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. As a result, the proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs.},
	urldate = {2021-01-08},
	journal = {arXiv:1712.05877 [cs, stat]},
	author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.05877},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Jacob et al. - 2017 - Quantization and Training of Neural Networks for E.pdf:/home/juho/Zotero/storage/TLCYKYS7/Jacob et al. - 2017 - Quantization and Training of Neural Networks for E.pdf:application/pdf},
}

@article{zhu_prune_2017,
	title = {To prune, or not to prune: exploring the efficacy of pruning for model compression},
	shorttitle = {To prune, or not to prune},
	url = {http://arxiv.org/abs/1710.01878},
	abstract = {Model pruning seeks to induce sparsity in a deep neural network{\textquoteright}s various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015a; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model{\textquoteright}s dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
	urldate = {2021-01-11},
	journal = {arXiv:1710.01878 [cs, stat]},
	author = {Zhu, Michael and Gupta, Suyog},
	month = nov,
	year = {2017},
	note = {arXiv: 1710.01878},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Zhu and Gupta - 2017 - To prune, or not to prune exploring the efficacy .pdf:/home/juho/Zotero/storage/N4VSIJB7/Zhu and Gupta - 2017 - To prune, or not to prune exploring the efficacy .pdf:application/pdf},
}
